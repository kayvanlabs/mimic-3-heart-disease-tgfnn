{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess MIMIC-III data for long-term heart failure prediction\n",
    "General workflow:\n",
    "1. get the features we want\n",
    "2. process categorical features\n",
    "3. process continuous features with lower recording frequencing\n",
    "4. process continuous features with higher recording frequencing\n",
    "5. combine it all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General set up and filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load files and set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mrmr import mrmr_classif\n",
    "from sklearn.impute import KNNImputer\n",
    "import os\n",
    "\n",
    "# column names\n",
    "ID = 'ID'\n",
    "TIME = 't'\n",
    "VAR = 'variable_name'\n",
    "VAL = 'variable_value'\n",
    "HF_LABEL = 'HF_LABEL'\n",
    "\n",
    "# Set the default plot size\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_pickle('data/input_data.p')\n",
    "labels = pd.read_csv('data/HF_240.0h.csv')\n",
    "df = pd.merge(df, labels[[ID, HF_LABEL]], on=ID) # map the patient heart failure label to the data\n",
    "\n",
    "# get names of features from dictionary tables\n",
    "d_items = pd.read_csv('data/D_ITEMS.csv')\n",
    "d_lab = pd.read_csv('data/D_LABITEMS.csv')\n",
    "\n",
    "# load in additional data\n",
    "icu_stays = pd.read_csv('data/ICUSTAYS.csv')\n",
    "dx = pd.read_csv('data/DIAGNOSES_ICD.csv')\n",
    "rx = pd.read_csv('data/PRESCRIPTIONS.csv')\n",
    "\n",
    "# map the ICUSTAY_ID to diagnoses\n",
    "id_map = icu_stays[icu_stays['ICUSTAY_ID'].isin(df[ID].unique())][['HADM_ID','ICUSTAY_ID']]\n",
    "dx = pd.merge(dx, id_map, on='HADM_ID')\n",
    "\n",
    "# format columns\n",
    "dx = dx.rename(columns={\n",
    "    'ICUSTAY_ID': ID,\n",
    "    'ICD9_CODE' : VAR\n",
    "})\n",
    "\n",
    "dx[VAL] = 1\n",
    "dx[TIME] = np.nan\n",
    "\n",
    "dx = dx[[ID, TIME, VAR, VAL]]\n",
    "dx = pd.merge(dx, labels[[ID, HF_LABEL]], on=ID)\n",
    "dx[VAR] = dx[VAR].apply(lambda x: 'ICD9: ' + x)\n",
    "\n",
    "\n",
    "rx = rx.rename(columns={\n",
    "    'ICUSTAY_ID': ID,\n",
    "    'DRUG' : VAR\n",
    "})\n",
    "\n",
    "rx[VAL] = 1\n",
    "rx[TIME] = np.nan\n",
    "\n",
    "rx = rx[[ID, TIME, VAR, VAL]]\n",
    "rx = pd.merge(rx, labels[[ID, HF_LABEL]], on=ID)\n",
    "rx[VAR] = rx[VAR].apply(lambda x: 'RX: ' + x)\n",
    "\n",
    "# combine data\n",
    "data = pd.concat([df, dx, rx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get features from literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prediction model of in-hospital mortality in intensive care unit patients with heart failure: \n",
    "machine learning-based, retrospective analysis of the MIMIC-III database\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8311359/\n",
    "'''\n",
    "\n",
    "demographics1 = ['AGE', 'GENDER', 'ETHNICITY', 'Weight', 'Height']\n",
    "vital_signs = ['HR', 'SysBP', 'DiaBP', 220181, 'RR', 'Temperature', 'SpO2']\n",
    "labs = [51221, 51493, 51222, 51250, 51277, 51265, 51274, 51237, 51006, 50931, 50971, 50983,\n",
    "        50893, 50902, 50960, 50868, 50882]\n",
    "\n",
    "'''\n",
    "Guo et al. An evaluation of time series summary statistics as features for clinical prediction tasks\n",
    "'''\n",
    "item_ids = [220739, 223901, 223900, 223835]\n",
    "lab_ids = [51301, 50971, 50822, 50821, 50882, 50983, 51006, 50885]\n",
    "demographic2 = ['AGE', 'ADMISSION_TYPE']\n",
    "\n",
    "'''\n",
    "Define our features types.\n",
    "These I assessed by hand.\n",
    "'''\n",
    "categorical = ['AGE', 'GENDER', 'ADMISSION_TYPE', 'ETHNICITY', 220739, 223900, 223901]\n",
    "continuous = ['HR', 'SysBP', 'DiaBP', 220181, 'RR', 'SpO2', 'Weight', 'Height', \n",
    "              51221, 51222, 51250, 51277, 51265, 51274, 51237, 51006, 50931, 50971, 50983, 51493,\n",
    "              50893, 50902, 50960, 50868, 50882, 223835, 51301, 50971, 50822, 50821, 50882,\n",
    "              50983, 51006, 50885]\n",
    "\n",
    "# make them strings (because the variable_name column in df is)\n",
    "categorical = [str(x) for x in categorical]\n",
    "continuous = [str(x) for x in continuous]\n",
    "\n",
    "# combine\n",
    "all_features = categorical + continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets by patient ID\n",
    "patient_ids = data[ID].unique()\n",
    "train_ids, test_ids = train_test_split(patient_ids, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rare features in training patients\n",
    "data_train = data[data[ID].isin(train_ids)]\n",
    "n_patients = data_train[ID].nunique()\n",
    "a = pd.DataFrame(data_train[[ID,VAR]].drop_duplicates().groupby(VAR)[ID].nunique()).reset_index()\n",
    "a.columns = [VAR, 'n_patients']\n",
    "a['n_patients'] = a['n_patients'] / n_patients\n",
    "a_rx_dx = a[a[VAR].str.contains('RX:|ICD9:')]\n",
    "a = a[~a[VAR].str.contains('RX:|ICD9:')]\n",
    "\n",
    "rare_features = a[a['n_patients'] <= 0.7][VAR].values.tolist()\n",
    "rare_dx_rx = a_rx_dx[a_rx_dx['n_patients'] <= 0.01][VAR].values.tolist()\n",
    "\n",
    "data_filtered = data[data[VAR].isin(rare_features + rare_dx_rx) == False]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate continuous and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate categorical features\n",
    "categorical.extend(data_filtered[data_filtered[VAR].str.contains('RX:|ICD9:')][VAR].unique().tolist())\n",
    "df_cat = data_filtered[data_filtered['variable_name'].isin(categorical)].reset_index(drop=True)\n",
    "df_cat = df_cat.drop_duplicates()\n",
    "\n",
    "# separate continuous features\n",
    "df_cont = data_filtered[data_filtered['variable_name'].isin(continuous)].reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many times each categorical feature is recorded for each patient\n",
    "cat_t_count = pd.DataFrame(df_cat.groupby(['ID','variable_name'])['t'].nunique()).reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and format demographics\n",
    "df_cat_demo = df_cat[df_cat['t'].isna()]\n",
    "\n",
    "# get ICD supergroups\n",
    "df_dx = df_cat_demo[df_cat_demo['variable_name'].str.contains('ICD9:')]\n",
    "df_dx['variable_name'] = df_dx['variable_name'].apply(lambda x: x.split('ICD9: ')[1])\n",
    "df_dx['variable_name'] = 'ICD9: ' + df_dx['variable_name'].str.slice(0,3)\n",
    "df_dx = df_dx.drop_duplicates()\n",
    "df_cat_demo = pd.concat([df_cat_demo, df_dx])\n",
    "df_cat_demo = df_cat_demo.drop_duplicates()\n",
    "\n",
    "df_cat_demo = df_cat_demo.pivot(index='ID', columns='variable_name', values='variable_value')\n",
    "\n",
    "# encode admission types on a scale\n",
    "admission_type_scale = {'ELECTIVE' : 1, 'URGENT' : 2, 'EMERGENCY' : 3}\n",
    "df_cat_demo['ADMISSION_TYPE'] = df_cat_demo['ADMISSION_TYPE'].map(admission_type_scale)\n",
    "\n",
    "# encode gender\n",
    "gender_scale = {'M' : 1, 'F' : 0}\n",
    "df_cat_demo['GENDER'] = df_cat_demo['GENDER'].map(gender_scale)\n",
    "df_cat_demo.rename(columns={'GENDER' : 'Female'}, inplace=True)\n",
    "\n",
    "# one hot encode ethnicity\n",
    "df_cat_demo = pd.get_dummies(df_cat_demo, columns=['ETHNICITY'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode categorical features according to Glascow Coma Scale\n",
    "https://www.cdc.gov/masstrauma/resources/gcs.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_opening_220739_scale = {'None' : 1, 'To Pain' : 2, 'To Speech' : 3, 'Spontaneously' : 4}\n",
    "verbal_223900_scale = {'No Response' : 1, 'No Response-ETT' : 1, 'Incomprehensible sounds' : 2, 'Inappropriate Words' : 3, 'Confused' : 4, 'Oriented' : 5}\n",
    "motor_223901_scale = {'No response' : 1, 'Abnormal extension' : 2, 'Abnormal Flexion' : 3, 'Flex-withdraws' : 4, 'Localizes Pain' : 5, 'Obeys Commands' : 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df_cat[df_cat['variable_name'] == '220739'].index\n",
    "df_cat.loc[idx, 'variable_value'] = df_cat.loc[idx, 'variable_value'].map(eye_opening_220739_scale).values\n",
    "\n",
    "idx = df_cat[df_cat['variable_name'] == '223900'].index\n",
    "df_cat.loc[idx, 'variable_value'] = df_cat.loc[idx, 'variable_value'].map(verbal_223900_scale).values\n",
    "\n",
    "idx = df_cat[df_cat['variable_name'] == '223901'].index\n",
    "df_cat.loc[idx, 'variable_value'] = df_cat.loc[idx, 'variable_value'].map(motor_223901_scale).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep mean GCS score\n",
    "df_cat_gcs = df_cat[df_cat['variable_name'].isin(['220739', '223900', '223901'])]\n",
    "df_cat_gcs = pd.DataFrame(df_cat_gcs.groupby(['ID','variable_name'])['variable_value'].mean()).reset_index()\n",
    "df_cat_gcs = df_cat_gcs.pivot(index='ID', columns='variable_name', values='variable_value') # pivot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_demo = df_cat_demo.fillna(0)\n",
    "\n",
    "df_cat_processed = pd.concat([df_cat_demo, df_cat_gcs], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_cont[VAL].str.contains('[a-zA-Z]', regex=True).fillna(False)\n",
    "df_cont = df_cont[~x]\n",
    "df_cont[VAL] = df_cont[VAL].str.strip()\n",
    "\n",
    "# Separate continuous features that are frequently sampled\n",
    "\n",
    "# see how many times each continuous feature is recorded for each patient\n",
    "cont_t_count = pd.DataFrame(df_cont.groupby(['ID','variable_name'])['t'].nunique()).reset_index()\n",
    "\n",
    "\n",
    "# based on this plot, I am only going to calculate all the summary statistics from features with more than 20 time points on average per patient\n",
    "# for all those with <20 times points per patient on average, I will just calculate a mean\n",
    "\n",
    "freq_cutoff = 20\n",
    "cont_mean_t_count = pd.DataFrame(df_cont.groupby(['ID','variable_name'])['t'].nunique().groupby('variable_name').mean()).reset_index().sort_values('t', ascending=False)\n",
    "high_freq_features = cont_mean_t_count[cont_mean_t_count['t'] >= freq_cutoff]['variable_name'].tolist()\n",
    "low_freq_features = cont_mean_t_count[cont_mean_t_count['t'] < freq_cutoff]['variable_name'].tolist()\n",
    "all_cont_features = high_freq_features + low_freq_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat values as floats\n",
    "df_cont_low_freq = df_cont[df_cont['variable_name'].isin(low_freq_features)]\n",
    "\n",
    "# convert values to floats (rbc = 51493)\n",
    "def to_float(x):\n",
    "    if isinstance(x, float) or isinstance(x, int):\n",
    "        return x\n",
    "    elif '-' in x:\n",
    "        return np.mean([float(y) for y in x.split('-')])\n",
    "    elif '>' in x or '<' in x:\n",
    "        return float(x[1:])\n",
    "    elif x == ' ' or 'ERROR' in x or 'UNABLE' in x or x.count('.') > 1 or x == '':\n",
    "        return np.nan\n",
    "    elif 'GREATER' in x:\n",
    "        return float(x.split(' ')[-1])\n",
    "    elif ': ' in x:\n",
    "        return float(x.split(': ')[-1])\n",
    "    else:\n",
    "        return float(x)\n",
    "\n",
    "df_cont_low_freq['variable_value'] = df_cont_low_freq['variable_value'].apply(lambda x: to_float(x))\n",
    "\n",
    "# get mean value for each patient and pivot\n",
    "df_cont_low_freq = pd.DataFrame(df_cont_low_freq.groupby(['ID','variable_name'])['variable_value'].mean()).reset_index()\n",
    "df_cont_low_freq = df_cont_low_freq.pivot(index='ID', columns='variable_name', values='variable_value')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute summary statistics for continuous features with high sampling frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat values as floats\n",
    "df_cont_high_freq = df_cont[df_cont['variable_name'].isin(all_cont_features)]\n",
    "df_cont_high_freq['variable_value'] = df_cont_high_freq['variable_value'].apply(lambda x: to_float(x))\n",
    "\n",
    "# compute mean and stdv\n",
    "df_cont_high_freq_mean = pd.DataFrame(df_cont_high_freq.groupby(['ID','variable_name'])['variable_value'].mean()).reset_index()\n",
    "df_cont_high_freq_std = pd.DataFrame(df_cont_high_freq.groupby(['ID','variable_name'])['variable_value'].std()).reset_index()\n",
    "df_cont_high_freq_mean['variable_name'] = df_cont_high_freq_mean['variable_name'] + '_mean'\n",
    "df_cont_high_freq_std['variable_name'] = df_cont_high_freq_std['variable_name'] + '_std'\n",
    "df_cont_high_freq_mean = df_cont_high_freq_mean.pivot(index='ID', columns='variable_name', values='variable_value')\n",
    "df_cont_high_freq_std = df_cont_high_freq_std.pivot(index='ID', columns='variable_name', values='variable_value')\n",
    "\n",
    "\n",
    "df_cont_high_freq = pd.concat([df_cont_high_freq_mean, df_cont_high_freq_std], axis=1) # df_cont_high_freq_variation, df_cont_high_freq_iqr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all data\n",
    "df_processed = pd.concat([df_cont_high_freq, df_cat_processed], axis=1) #df_cont_low_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import icd9cms.icd9 as icd9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature names\n",
    "cols = df_processed.columns\n",
    "d_icd = pd.read_csv('data/D_ICD_DIAGNOSES.csv')\n",
    "\n",
    "for col in cols:\n",
    "\n",
    "    # init vars\n",
    "    id = None\n",
    "    label = None\n",
    "    has_stat_suffix = False\n",
    "    icd = False\n",
    "\n",
    "    # check if feature is ID we can map to feature name\n",
    "    if col.isdigit():\n",
    "        id = int(col)\n",
    "    elif '_' in col:\n",
    "        feature = col.split('_')[0]\n",
    "        if feature.isdigit():\n",
    "            id = int(feature)\n",
    "            has_stat_suffix = True\n",
    "    elif 'ICD9:' in col:\n",
    "        id = col.split('ICD9: ')[-1]\n",
    "        icd = True\n",
    "\n",
    "    # find feature label in tables and update\n",
    "    if id is not None:\n",
    "        if icd:\n",
    "            if id in d_icd['ICD9_CODE'].values:\n",
    "                label = d_icd[d_icd['ICD9_CODE'] == id]['LONG_TITLE'].values[0]\n",
    "                label = 'DX: ' + label + f' ({id})'\n",
    "            elif icd9.search(id):\n",
    "                label = 'DX: ' + icd9.search(id).short_desc + f' ({id})'\n",
    "            icd = False\n",
    "        if id in d_items['ITEMID'].values:\n",
    "            label = d_items[d_items['ITEMID'] == id]['LABEL'].values[0]\n",
    "        elif id in d_lab['ITEMID'].values:\n",
    "            label = d_lab[d_lab['ITEMID'] == id]['LABEL'].values[0]\n",
    "        \n",
    "        if label is not None:\n",
    "            if has_stat_suffix:\n",
    "                df_processed.rename(columns={col: label + '_' + col.split('_')[1]}, inplace=True)\n",
    "            else:\n",
    "                df_processed.rename(columns={col: label}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move and format ID column\n",
    "df_processed = df_processed.reset_index()\n",
    "df_processed['ID'] = df_processed['ID'].astype(int)\n",
    "\n",
    "# get target variable\n",
    "labels['ID'] = labels['ID'].astype(int)\n",
    "df_processed = pd.merge(df_processed, labels[['ID','HF_LABEL']], on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove diagnosis and medication features\n",
    "leaky_cols = ['Do not resuscitate status (V4986)','Encounter for palliative care (V667)', 'Convalescence and palliative care (V66)']\n",
    "df_processed = df_processed[df_processed.columns[~df_processed.columns.isin(leaky_cols)]]\n",
    "# df_processed = df_processed[df_processed.columns[~df_processed.columns.str.contains('RX:|DX:')]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets by patient ID\n",
    "x_train = df_processed[df_processed[ID].isin(train_ids)].drop(columns=[ID,HF_LABEL])\n",
    "x_test = df_processed[df_processed[ID].isin(test_ids)].drop(columns=[ID,HF_LABEL])\n",
    "y_train = df_processed[HF_LABEL][df_processed[ID].isin(train_ids)]\n",
    "y_test = df_processed[HF_LABEL][df_processed[ID].isin(test_ids)]\n",
    "\n",
    "# get complete processed dataset with IDs, HF labels, and train/test labels\n",
    "df_processed['TRAIN'] = df_processed[ID].isin(train_ids) * 1\n",
    "\n",
    "x_train = x_train.astype(float)\n",
    "x_test = x_test.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection and imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "selected_features, relvence_scores, redundance_scores = mrmr_classif(x_train.astype(float), y_train, K=30, return_scores=True, show_progress=True)\n",
    "x_train = x_train[selected_features]\n",
    "x_test = x_test[selected_features]\n",
    "\n",
    "# impute missing values with knn\n",
    "imputer = KNNImputer(n_neighbors=9, weights='distance')\n",
    "x_train = pd.DataFrame(imputer.fit_transform(x_train), columns=x_train.columns)\n",
    "x_test = pd.DataFrame(imputer.transform(x_test), columns=x_test.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'data_nolowfreq_statsmeanstd_mrmr30_knn9distweight_noleakage'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "x_train.to_csv(f'{out_dir}/x_train.csv', index=False)\n",
    "x_test.to_csv(f'{out_dir}/x_test.csv', index=False)\n",
    "y_train.to_csv(f'{out_dir}/y_train.csv', index=False)\n",
    "y_test.to_csv(f'{out_dir}/y_test.csv', index=False)\n",
    "df_processed.to_csv(f'{out_dir}/df_processed.csv', index=False)\n",
    "data_filtered.to_csv(f'{out_dir}/data_filtered.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
