{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generalized_fuzzy_net import GeneralizedFuzzyClassifier as TGFNN\n",
    "from rule_extraction import extract_rules_from_net\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from load_dataset import load_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import icd9cms.icd9 as icd9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_index(concepts, concept_thresholds):\n",
    "    '''\n",
    "    Use this function to adjust the names of concepts in the output rule table to make them more interpretable.\n",
    "    '''\n",
    "    new_concepts = []\n",
    "\n",
    "    for concept in concepts:\n",
    "\n",
    "        if concept == 'directions' or concept.startswith('weight_'):\n",
    "            new_concepts.append(concept)\n",
    "            continue\n",
    "\n",
    "        # make quantity more legible\n",
    "        quantity = concept.split('_')[-1]\n",
    "        concept = ' '.join(concept.split('_')[:-1]) #.title()\n",
    "\n",
    "        # process ICD codes\n",
    "        if concept.endswith(')') and 'RX:' not in concept:\n",
    "            code = concept.split('(')[-1].split(')')[0]\n",
    "            if icd9.search(code).long_desc:\n",
    "                desc = icd9.search(code).long_desc\n",
    "            else:\n",
    "                desc = icd9.search(code).short_desc\n",
    "            concept = '\\\"' + desc + '\\\" (' + code + ')'\n",
    "\n",
    "        if quantity == 'level0':\n",
    "            concept = 'No ' + concept\n",
    "        elif quantity == 'level1':\n",
    "            concept = concept\n",
    "        else:\n",
    "            concept = concept + ' is ' + quantity\n",
    "\n",
    "        # specific adjustments\n",
    "        swap = {\n",
    "            'Mmhg' : 'mmHg',\n",
    "            'Cvp' : 'CVP',\n",
    "            'Map' : 'MAP',\n",
    "            'Hr' : 'hr',\n",
    "            'Fluid Based' : '(fluid based)',\n",
    "            'Arterial Line' : '(arterial line)',\n",
    "            'Mpv' : 'MPV',\n",
    "            'AGE' : 'Age',\n",
    "            'RDW' : 'Red Cell Distribution Width',\n",
    "            'No ETHNICITY ASIAN' : 'Not of Asian ethnicity',\n",
    "            'ETHNICITY ASIAN' : 'of Asian ethnicity',\n",
    "            '(100 units/ml)' : '',\n",
    "            'RX: ' : '',\n",
    "            'Antineoplastic and immunosuppressive drugs causing adverse effects in therapeutic use' : 'Adverse effects of antineoplastic/immunosuppressive drugs',\n",
    "            'Bacterial infection in conditions classified elsewhere and of unspecified site' : 'Other Bacterial Infection',\n",
    "            'Persistent mental disorders due to conditions classified elsewhere' : 'Other persistent mental disorders',\n",
    "            'Secondary malignant neoplasm of respiratory and digestive systems' : 'Secondary malignant neoplasm of respiratory/digestive systems'\n",
    "        }\n",
    "\n",
    "        for key in swap.keys():\n",
    "            concept = concept.replace(key, swap[key])\n",
    "        \n",
    "\n",
    "        new_concepts.append(concept)\n",
    "\n",
    "    return new_concepts\n",
    "\n",
    "\n",
    "def del_vars(x, category_info, threshhold):\n",
    "    n_cont = len(category_info[category_info == 0])\n",
    "    n_cat = len(category_info[category_info > 0])\n",
    "\n",
    "    x_cont = x.iloc[:n_cont*3,:]\n",
    "    x_cat = x.iloc[n_cont*3:,:]\n",
    "\n",
    "    # remove continuous variables with max value less than threshhold\n",
    "    for i in range(n_cont):\n",
    "        max = x_cont.iloc[i*3:(i+1)*3,:].max(axis=1).max()\n",
    "        if max < threshhold:\n",
    "            x_cont = x_cont.drop(x_cont.index[i*3:(i+1)*3])\n",
    "\n",
    "    # remove categorical variables with max value less than threshhold\n",
    "    for i in range(n_cat):\n",
    "        max = x_cat.iloc[i*2:(i+1)*2,:].max(axis=1).max()\n",
    "        if max < threshhold:\n",
    "            x_cat = x_cat.drop(x_cat.index[i*2:(i+1)*2])\n",
    "\n",
    "    return pd.concat([x_cont, x_cat])\n",
    "\n",
    "\n",
    "def get_thresholds(rules, params, encoding_value_details):\n",
    "    rule_var_name = rules.index\n",
    "    rule_var_name = set([name.split('_low')[0] for name in rule_var_name if 'low' in name])\n",
    "\n",
    "    continous_variable_name = [params.feature_names[index] \\\n",
    "                                for index in range(len(params.category_info)) \\\n",
    "                                if params.category_info[index]==0]\n",
    "\n",
    "    encoding = pd.DataFrame(np.transpose(encoding_value_details), index=continous_variable_name,\n",
    "                            columns=['low', 'medium_left', 'medium_right', 'high'])\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'cv_results'\n",
    "exp = 'run-11_newcontraregu'\n",
    "n_repeats = 1\n",
    "dataset = pickle.load(open(f'{dir}/{exp}/dataset.pkl', 'rb'))\n",
    "model = pickle.load(open(f'{dir}/{exp}/{n_repeats}_trained_GFN.mdl', 'rb'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(object):\n",
    "    binary_pos_only = model.binary_pos_only\n",
    "    n_rules = model.n_rules\n",
    "    n_classes = model.n_classes\n",
    "    category_info = dataset['category_info']\n",
    "    epsilon1 = model.min_epsilon1\n",
    "    epsilon2 = model.min_epsilon2\n",
    "    epsilon3 = model.min_epsilon3\n",
    "    feature_names = dataset.get('feature_names')\n",
    "    \n",
    "params = Params()\n",
    "\n",
    "# Extract rule data from a trained model\n",
    "rules, rules_all, _, _, encoding_value_details = extract_rules_from_net(\n",
    "    model.estimator, \n",
    "    params, \n",
    "    model.scaler, \n",
    "    keep_irrelevant_variable=True,\n",
    "    filter_similarity_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get thresholds\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "encoding = get_thresholds(rules, params, encoding_value_details)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rules_all.drop(columns=['encoding'])\n",
    "x = x[x.loc['pos_class_contribution',:].sort_values(ascending=False).index.tolist()]\n",
    "x.columns = ['R' + str(i) for i in range(1, x.shape[1]+1)]\n",
    "x.columns = (x.loc['pos_class_contribution',:].index + '\\n' + x.loc['pos_class_contribution',:].astype(str)).values.tolist()\n",
    "x = x.iloc[:-1,:]\n",
    "\n",
    "# # remove variables with max value less than 0.1\n",
    "x = del_vars(x, dataset['category_info'], threshhold=0.1)\n",
    "\n",
    "# make index more readable\n",
    "x.index = adjust_index(x.index.tolist(), encoding)\n",
    "\n",
    "plt.clf()\n",
    "sns.set(font_scale=1.5)\n",
    "plt.figure(figsize=(23,17))\n",
    "g = sns.heatmap(x, cmap='Reds', linewidths=0.75, linecolor='white', cbar_kws={'shrink': 0.5}) #sns.light_palette('#F08080', 10)\n",
    "g.figure.tight_layout()\n",
    "g.set_xticklabels(g.get_xmajorticklabels(), fontweight = 'bold')\n",
    "# plt.show()\n",
    "plt.savefig(f'{dir}/{exp}/figure_rules_all_rules.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = {}\n",
    "\n",
    "# write rules\n",
    "def write(k, v, rules):\n",
    "    # low < encoding\n",
    "    if v == ['low']:\n",
    "        val = rules[(rules['feature'] == k) & (rules['concept'].isin(v))]['encoding'].values[0]\n",
    "        return f'{k} < {val}{units.get(k,\"\")}'\n",
    "\n",
    "    # high > encoding\n",
    "    if v == ['high']:\n",
    "        val = rules[(rules['feature'] == k) & (rules['concept'].isin(v))]['encoding'].values[0]\n",
    "        return f'{k} > {val}{units.get(k,\"\")}'\n",
    "\n",
    "    # medium > low and < high\n",
    "    if v == ['medium']:\n",
    "        val_l = rules[(rules['feature'] == k) & (rules['concept'] == 'low')]['encoding'].values[0]\n",
    "        val_h = rules[(rules['feature'] == k) & (rules['concept'] == 'high')]['encoding'].values[0]\n",
    "        return f'{k} > {val_l} and < {val_h}{units.get(k,\"\")}'\n",
    "\n",
    "    # low and high < low and > high\n",
    "    if v == ['low', 'high']:\n",
    "        val_l = rules[(rules['feature'] == k) & (rules['concept'] == 'low')]['encoding'].values[0]\n",
    "        val_h = rules[(rules['feature'] == k) & (rules['concept'] == 'high')]['encoding'].values[0]\n",
    "        return f'{k} < {val_l} or > {val_h}{units.get(k,\"\")}'\n",
    "\n",
    "    # low and medium < medium\n",
    "    if v == ['low','medium']:\n",
    "        val = rules[(rules['feature'] == k) & (rules['concept'] == 'medium')]['encoding'].values[0]\n",
    "        return f'{k} < {val}{units.get(k,\"\")}'\n",
    "\n",
    "    # high and medium > medium\n",
    "    if v == ['medium', 'high']:\n",
    "        val = rules[(rules['feature'] == k) & (rules['concept'] == 'medium')]['encoding'].values[0]\n",
    "        return f'{k} > {val}{units.get(k,\"\")}'\n",
    "    \n",
    "    if v == ['level0']:\n",
    "        return f'{k} is absent'\n",
    "\n",
    "    if v == ['level1']:\n",
    "        return f'{k} is present'\n",
    "\n",
    "# prep data\n",
    "rules = rules_all[rules_all.loc['pos_class_contribution',:].sort_values(ascending=False).index.tolist()]\n",
    "rules = rules.drop('pos_class_contribution').reset_index(names='concept')\n",
    "concepts = pd.DataFrame([x.rsplit('_', 1) for x in rules['concept'].tolist()], columns=['feature','concept'])\n",
    "rules = rules.drop(columns=['concept'])\n",
    "rules = pd.concat([concepts, rules], axis=1)\n",
    "\n",
    "# process rule\n",
    "for rule in rules.drop(['feature','concept','encoding'], axis=1).columns:\n",
    "    x = rules[rules[rule] > 0.1][['feature','concept','encoding',rule]] # threshold\n",
    "    concepts = x.groupby('feature')['concept'].apply(list).to_dict()\n",
    "\n",
    "    written_rule = ''\n",
    "    for k,v in concepts.items():\n",
    "        written_rule = written_rule + write(k,v,rules) + ' and '\n",
    "\n",
    "    written_rule = written_rule[:-5] # remove last ' and '\n",
    "    print(f'{rule}: {written_rule}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
